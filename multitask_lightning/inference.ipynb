{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import cv2\n",
    "import yaml\n",
    "import random\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "import torch\n",
    "\n",
    "from models.model import UnetClipped\n",
    "from metrics.utils import masks_to_bboxes\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/train_config.yaml', 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary params from config \n",
    "image_size = config[\"train_data_params\"][\"size\"]\n",
    "stride = config[\"train_data_params\"][\"output_stride\"]\n",
    "num_classes = sum(config[\"train_data_params\"][\"subclasses\"])\n",
    "out_img_size = image_size // stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model, load state dict\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = UnetClipped(**config[\"model\"])\n",
    "\n",
    "ckpt_path = \"checkpoints/epoch=024-val_loss=0.912-val_map=0.723.ckpt\"\n",
    "state_dict = torch.load(ckpt_path)['state_dict']\n",
    "fixed_state_dict = {key.replace('net.', ''): value for key, value in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(fixed_state_dict)\n",
    "model = model.cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = \"test_videos/video\"\n",
    "dist_folder = \"test_videos/video_predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(num_classes)]\n",
    "\n",
    "# define augmentations\n",
    "resize_aug = [albu.SmallestMaxSize(max_size=image_size, always_apply=True),\n",
    "              albu.PadIfNeeded(min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, border_mode=0)]\n",
    "norm_aug = [albu.Normalize(mean=[0.449, 0.449, 0.449], std=[0.226, 0.226, 0.226])]\n",
    "\n",
    "resize_pipeline = albu.Compose(resize_aug, p=1)\n",
    "preproc_pipeline = albu.Compose(resize_aug + norm_aug, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for video_name in os.listdir(source_folder):\n",
    "    \n",
    "    print(f\"{video_name} is being processed\")\n",
    "    \n",
    "    video_path = os.path.join(source_folder, video_name)\n",
    "    dist_path = os.path.join(dist_folder, os.path.splitext(video_name)[0] + \"_predict.mp4\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'FMP4')\n",
    "    out = None\n",
    "    \n",
    "    current_frame = 0\n",
    "    while(cap.isOpened()):\n",
    "            \n",
    "        ret, img = cap.read()  \n",
    "        \n",
    "        try:\n",
    "            h, w, _ = img.shape\n",
    "        except:\n",
    "            break\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  \n",
    "        \n",
    "        # prepare model input\n",
    "        model_input = preproc_pipeline(image=img)[\"image\"]\n",
    "        model_input = torch.from_numpy(model_input.transpose((2, 0, 1))).float().unsqueeze(0)\n",
    "        output = model(model_input.cuda())\n",
    "        \n",
    "        # resize image\n",
    "        img_result = resize_pipeline(image=img)[\"image\"]\n",
    "        \n",
    "        # draw detected boxes\n",
    "        bboxes = masks_to_bboxes(output, num_classes, max_bbox_per_img=5, threshold=0.4, out_size=out_img_size, is_predict=True)      \n",
    "        for box in bboxes[0]:\n",
    "            x1, y1, x2, y2 = 2 * box[:4].astype(int)\n",
    "            class_id = box[4].astype(int)\n",
    "            if class_id not in [2]:\n",
    "                cv2.rectangle(img_result, (x1, y1), (x2, y2), colors[class_id], 2)\n",
    "        \n",
    "        # add frame to the final video\n",
    "        img_result = cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB) \n",
    "        if out is None:\n",
    "            out = cv2.VideoWriter(dist_path, fourcc, int(fps), (img_result.shape[1], img_result.shape[0]), True)\n",
    "        out.write(np.uint8(img_result))\n",
    "        \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"{video_name} is finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleanup",
   "language": "python",
   "name": "cleanup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
